{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw5_mc_01_v02.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TALeonard/19ma573thomasleonard/blob/master/src/hw5_mc_01_v02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CmAEiE472_FB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Assignment Outline\n",
        "Given i.i.d $\\{\\alpha_i: i\\in 1, 2, \\ldots, N\\}$, we use \n",
        "$$\\bar \\alpha = \\frac 1 N \\sum_{i=1}^N \\alpha_i$$\n",
        "as its estimator of the mean $\\mathbb E[\\alpha_1]$ \n",
        "and \n",
        "$$\\beta_N = \\frac 1 N \\sum_{i=1}^N (\\alpha_i -\\bar \\alpha)^2$$\n",
        "as the estimator of $Var(\\alpha_1)$.\n",
        "\n",
        "\n",
        "Suppose $\\alpha_1\\in L^2$, then \n",
        "- Prove $\\beta_N$ is biased.\n",
        "- Prove that $\\beta_N$ is consistent in $L^2$.\n",
        "- Can you propose an unbiased estimator?"
      ]
    },
    {
      "metadata": {
        "id": "1v2c-KJE3E7j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Show the Bias of $\\beta_{N}$\n",
        "First, we wish to show that $\\beta_{N}$ is biased. If this is biased, then $E[\\beta_{N}] - Var(\\alpha_{1}) \\neq 0$.\n",
        "\n",
        "$$E[\\beta_{N}] = E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\bar\\alpha)^{2}]$$\n",
        "\n",
        "Consider subtracting $\\mu = E[\\alpha_{1}]$ from both $\\alpha_{i}$ and $\\bar\\alpha$.\n",
        "\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}((\\alpha_{i}-\\mu)-(\\bar\\alpha-\\mu))^{2}]$$\n",
        "\n",
        "Now, expand the square.\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}((\\alpha_{i}-\\mu)^{2}-2(\\alpha_{i}-\\mu)(\\bar\\alpha-\\mu)+(\\bar\\alpha-\\mu)^{2})]$$\n",
        "\n",
        "Distribute the $\\frac{1}{N}$ term and split into 3 summations. Note that $\\bar\\alpha - \\mu$ is a constant value with respect to $i$, and can thus be taken out of the summation where applicable:\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-\\frac{2}{N}(\\bar\\alpha-\\mu)\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)+\\frac{1}{N}(\\bar\\alpha-\\mu)^{2}\\sum_{i=1}^{N}1]$$\n",
        "\n",
        "The last summation is simply $N$ copies of $1$, and is thus $N$. This cancels the fraction coefficient on the last term:\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-\\frac{2}{N}(\\bar\\alpha-\\mu)\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)+(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "To continue, we will have to consider the definition of $\\bar\\alpha$. I will opt to introduce a subtraction of $\\mu$ to both sides, as this can be folded into the summation (which will allow us to reduce at least one of these summations). My end goal here is going to be writing something not unlike $E[(X-\\mu)^{2}]$, which is the definition of $Var(X)$ (here it would obviously be in terms of $\\alpha$, but the intent and result would be the same):\n",
        "\n",
        "$$\\bar\\alpha-\\mu=\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}) - \\mu$$\n",
        "\n",
        "$$\\bar\\alpha-\\mu=\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\mu$$\n",
        "\n",
        "The second line is true as $\\mu$ is constant with respect to a summation over $i$, thus a summation from $1$ to $N$ will simply be $N*\\mu$; thus, dividing this summation by $N$ will yield the original term $\\mu$. Now, combine the two terms:\n",
        "\n",
        "$$\\bar\\alpha-\\mu=\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)$$\n",
        "\n",
        "This will allow us to simplify the middle term in the expectation of $\\beta_{N}$:\n",
        "\n",
        "$$E[\\beta_{N}]=E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-2(\\bar\\alpha-\\mu)*\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)+(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-2(\\bar\\alpha-\\mu)*(\\bar\\alpha-\\mu)+(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-2(\\bar\\alpha-\\mu)^{2}+(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "By the properties of the expectation, we can split these terms:\n",
        "\n",
        "$$=E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}]-E[(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "Note that the first expectaiton is a direct definition of the variance itself, $\\sigma^{2}$. By pulling the fraction coefficient and the summation out of the expectation, each inside term is the variance of $'\\alpha_{i}$. Since all of the $\\alpha_{i}$ terms are i.i.d., they share an expectation, thus all have the same variance $\\sigma^{2}$.\n",
        "\n",
        "$$E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}]=\\frac{1}{N}\\sum_{i=1}^{N}E[(\\alpha_{i}-\\mu)^2]=\\frac{1}{N}\\sum_{i=1}^{N}\\sigma^{2}=\\frac{1}{N}*\\sigma^{2}*N=\\sigma^{2}$$\n",
        "\n",
        "Further, note that the second term is the variance of $\\bar\\alpha$, which we can simplify as follows:\n",
        "\n",
        "$$E[(\\bar\\alpha-\\mu)^{2}]=Var(\\bar\\alpha)=Var(\\frac{1}{N}\\sum_{i=1}^{N}\\alpha_{i})=\\frac{1}{N^{2}}Var(\\sum_{i=1}^{N}\\alpha_{i})=\\frac{1}{N^{2}}*N*Var(\\alpha_{1})=\\frac{1}{N}\\sigma^{2}$$\n",
        "\n",
        "Thus, we can write the expectation from 2 lines up in the following way:\n",
        "\n",
        "$$E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}]-E[(\\bar\\alpha-\\mu)^{2}]=\\sigma^{2}-\\frac{\\sigma^{2}}{N}=(1-\\frac{1}{N})\\sigma^{2}\\neq\\sigma^{2}=Var(\\alpha_{1})$$\n",
        "\n",
        "Thus, the bias of $\\beta_{N}$ is nonzero, and this is a biased estimator of the variance."
      ]
    },
    {
      "metadata": {
        "id": "2MvjN7BvCsxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Is $\\beta_{N}$ Consistent?\n",
        "\n",
        "Now, we want to show that $\\beta_{N}$ is consistent in $L^{2}$. What we want to do is show that \n",
        "\n",
        "$$E[(\\beta_{N}-\\alpha)^{2}] \\rightarrow 0$$ where $\\alpha$ is the underlying value we are trying to estimate; here, that is the variance $\\sigma^{2}$.\n",
        "\n",
        "Thus, our goal is to see if the Mean Square Error (MSE) of this goes to $0$ as $N$ goes to $\\infty$. From the lecture, we have a proposition that we can define the MSE in the following way:\n",
        "\n",
        "$$MSE(\\beta_{N})=|Bias(\\beta_{N})|^{2} + Var(\\beta_{N})$$\n",
        "\n",
        "From the last line of the above proof, we have that \n",
        "\n",
        "$$Bias(\\beta_{N}) = \\sigma^{2}(1-\\frac{1}{N})-\\sigma^{2} = \\frac{-\\sigma^{2}}{N}$$\n",
        "\n",
        "Next, we need the Variance of $\\beta_{N}$. Given this is a \"standard\" sample variance estimator, we can research this online to save ourselves from having to compute it by hand. My derivation would essentially be based on [the derivation of this found on Wolfram's website](http://mathworld.wolfram.com/SampleVarianceDistribution.html), and yields the following result:\n",
        "\n",
        "$$Var(\\beta_{N}) = \\frac{(N-1)^{2}}{N^{3}}*E[(\\alpha-E[\\alpha])^{4}]-\\frac{(N-1)(N-3)\\sigma^{4}}{N^{3}}$$\n",
        "\n",
        "Note that this requires that the fourth central moment exists. If it does, we can compute this value. We cannot say if it does or not based on the outline of the assignment (as we are only given that $\\alpha_{1}\\in L^{2}$, which does not necessarily mean that $\\alpha_{1}\\notin L^{4}$). For the purpose of this exercise, I will assume that the fourth central moment does exist. Based on this, let us expand the MSE equation now:\n",
        "\n",
        "$$MSE(\\beta_{N})=|Bias(\\beta_{N})|^{2} + Var(\\beta_{N})$$\n",
        "$$= (\\frac{\\sigma^{2}}{N})^{2} + \\frac{(N-1)^{2}}{N^{3}}*E[(\\alpha-E[\\alpha])^{4}]-\\frac{(N-1)(N-3)\\sigma^{4}}{N^{3}}$$\n",
        "\n",
        "$$= (\\frac{\\sigma^{4}}{N^{2}}) + \\frac{N-1}{N^{3}}((N-1)*E[(\\alpha-E[\\alpha])^{4}]-(N-3)\\sigma^{4})$$\n",
        "\n",
        "Note that in each term, everything remaining is now a fixed value. The expectation is the fourth central moment of $\\alpha$, which is known (under the assumption that it exists). The $\\sigma$ terms are constants, representing the square of the variance of $\\alpha$. Thus, as $N$ goes to $\\infty$, each of these terms will go to 0 (since the $N$'s in the denominators will remain even after potential simplification, such as by L'Hopital's Rule). Thus, $\\beta_{N}$ is consistent in $L^{2}$."
      ]
    },
    {
      "metadata": {
        "id": "QJFdKoPaLQ0m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Proposal of an Unbiased Estimator\n",
        "\n",
        "Now, we shall propose an unbiased estimator for the variance. This estimator is a well-known correction to \"classical\" sample variance in order to fix the biased nature of it. Denote the new estimator as $\\hat\\beta_{N}$.\n",
        "\n",
        "$$\\hat\\beta_{N} = \\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\bar\\alpha)^{2}$$\n",
        "\n",
        "I will now show that this is an unbiased estimator. As a lot of the steps are the same, I will be omitting the explanations along the way (except where steps differ).\n",
        "\n",
        "$$=E[\\frac{1}{N-1}\\sum_{i=1}^{N}((\\alpha_{i}-\\mu)-(\\bar\\alpha-\\mu))^{2}]$$\n",
        "\n",
        "$$=E[\\frac{1}{N-1}\\sum_{i=1}^{N}((\\alpha_{i}-\\mu)^{2}-2(\\alpha_{i}-\\mu)(\\bar\\alpha-\\mu)+(\\bar\\alpha-\\mu)^{2})]$$\n",
        "\n",
        "$$=E[\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-\\frac{2}{N-1}(\\bar\\alpha-\\mu)\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)+\\frac{1}{N-1}(\\bar\\alpha-\\mu)^{2}\\sum_{i=1}^{N}1]$$\n",
        "\n",
        "Here, the last fraction does not cancel.\n",
        "\n",
        "$$=E[\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-\\frac{2}{N-1}(\\bar\\alpha-\\mu)\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)+\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "As before, we now consider the definition of $\\bar\\alpha$. Here, we will have to now simplify the right-hand side by removing the fraction coefficient from it.\n",
        "\n",
        "$$\\bar\\alpha-\\mu=\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}) - \\mu$$\n",
        "\n",
        "$$\\bar\\alpha-\\mu=\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i})-\\frac{1}{N}\\sum_{i=1}^{N}\\mu$$\n",
        "\n",
        "$$N*(\\bar\\alpha-\\mu)=\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)$$\n",
        "\n",
        "This will allow us to simplify the middle term in the expectation of $\\hat\\beta_{N}$:\n",
        "\n",
        "$$E[\\hat\\beta_{N}]=E[\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-2(\\bar\\alpha-\\mu)*\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)+\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "$$=E[\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-\\frac{2N}{N-1}(\\bar\\alpha-\\mu)^{2}+\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "$$=E[\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}-\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "By the properties of the expectation, we can split these terms:\n",
        "\n",
        "$$=E[\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}]-E[\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]$$\n",
        "\n",
        "If we multiply the first expectation by $\\frac{N}{N}$ then it will contain our definition of variance from the earlier proof.\n",
        "\n",
        "$$E[\\frac{N}{N-1}*\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}]=\\frac{N}{N-1}E[\\frac{1}{N}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}]=\\frac{N}{N-1}\\sigma^{2}$$\n",
        "\n",
        "The fraction in the second term can be extraced from the expectation and then the process is the same as it was before:\n",
        "\n",
        "$$E[\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]=\\frac{N}{N-1}E[(\\bar\\alpha-\\mu)^{2}]=\\frac{N}{N-1}Var(\\bar\\alpha)$$\n",
        "\n",
        "$$Var(\\bar\\alpha)=Var(\\frac{1}{N}\\sum_{i=1}^{N}\\alpha_{i})=\\frac{1}{N^{2}}Var(\\sum_{i=1}^{N}\\alpha_{i})=\\frac{1}{N^{2}}*N*Var(\\alpha_{1})=\\frac{1}{N}\\sigma^{2}$$\n",
        "\n",
        "$$\\rightarrow E[\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]=\\frac{N}{N-1}*\\frac{1}{N}\\sigma^{2}=\\frac{1}{N-1}\\sigma^{2}$$\n",
        "\n",
        "Thus, we can conclude our result:\n",
        "\n",
        "$$E[\\frac{1}{N-1}\\sum_{i=1}^{N}(\\alpha_{i}-\\mu)^{2}]-E[\\frac{N}{N-1}(\\bar\\alpha-\\mu)^{2}]=\\frac{N}{N-1}\\sigma^{2}-\\frac{1}{N-1}\\sigma^{2}=\\frac{N-1}{N-1}\\sigma^{2}=\\sigma^{2}$$\n",
        "\n",
        "Thus, the bias of $\\hat\\beta_{N}$ is zero since its expected value is the variance of the underlying random variable, making this an unbiased estimator of variance."
      ]
    }
  ]
}